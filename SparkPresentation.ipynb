{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Situation\n",
    "Data growing faster than your desktop computer can handle.\n",
    "\n",
    "Know of Spark. Think that adding nodes is an easier way to handle data growth than to buy a faster computer.\n",
    "\n",
    "Want to test Spark to show your boss why it'll work for your company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Local machine\n",
    "Find a blog that helps you to get started on your local machine.\n",
    "\n",
    "Test out data frames to see how the language works.\n",
    "\n",
    "[Blog Link](http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###RDD\n",
    "Definition: \n",
    "- A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel.\n",
    "- Human terms:\n",
    "    - An RDD is lazy evaluation that allows Spark to determine the best way to complete the request.\n",
    "    - Spark executes the set of instructions on the RDD when an action is called.\n",
    "\n",
    "\n",
    "Action\n",
    "- Example actions:\n",
    "    - count()\n",
    "    - collect()\n",
    "    - take()\n",
    "    - first()\n",
    "\n",
    "\n",
    "- [link to Docs on RDD](https://spark.apache.org/docs/1.5.0/programming-guide.html#rdd-operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = 1\n",
    "\n",
    "# partitions - the number of \"groupings\" Spark cuts the dataset into\n",
    "# Recommended - 2-4 partitions per node\n",
    "partitions = nodes * 4\n",
    "training = sc.textFile('inmar/weather/2007_small.csv', partitions)\n",
    "training = training.map(lambda x: x.split(','))\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'CA002303986', u'20070101', u'TMAX', u'-130', u'', u'', u'G', u''],\n",
       " [u'CA002303986', u'20070101', u'TMIN', u'-220', u'', u'', u'G', u''],\n",
       " [u'CA002303986', u'20070101', u'PRCP', u'0', u'T', u'', u'G', u'']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc is an existing SparkContext.\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "rows = sc.textFile('inmar/weather/2007_small.csv')\n",
    "parts = rows.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the schema for the data\n",
    "schema_weather = parts.map(lambda p: Row(station=str(p[0]), date=str(p[1]), element=str(p[2]), \n",
    "                                         value=float(p[3]), measurement=str(p[4]), quality=str(p[5]), \n",
    "                                         source= str(p[5]), hour = str(p[7])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Infer the schema, and register the DataFrame as a table.\n",
    "weather = sqlContext.createDataFrame(schema_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Take a look at the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+-----------+-------+------+-----------+------+\n",
      "|    date|element| hour|measurement|quality|source|    station| value|\n",
      "+--------+-------+-----+-----------+-------+------+-----------+------+\n",
      "|20070101|   TMAX|     |           |       |      |CA002303986|-130.0|\n",
      "|20070101|   TMIN|     |           |       |      |CA002303986|-220.0|\n",
      "|20070101|   PRCP|     |          T|       |      |CA002303986|   0.0|\n",
      "|20070101|   PRCP|     |           |       |      |ASN00037003|   0.0|\n",
      "|20070101|   TMAX|     |           |       |      |NOE00133566|  66.0|\n",
      "|20070101|   TMIN|     |           |       |      |NOE00133566|  34.0|\n",
      "|20070101|   PRCP|     |           |       |      |NOE00133566| 151.0|\n",
      "|20070101|   TMAX|800.0|           |       |      |USC00242347|  89.0|\n",
      "|20070101|   TMIN|800.0|           |       |      |USC00242347| -39.0|\n",
      "|20070101|   PRCP|800.0|          P|       |      |USC00242347|   0.0|\n",
      "+--------+-------+-----+-----------+-------+------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###How many rows are there in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49999"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Unique values in the Elements column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|element|\n",
      "+-------+\n",
      "|   TMAX|\n",
      "|   TMIN|\n",
      "|   PRCP|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather[['element']].distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Max temp look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             value|\n",
      "+-------+------------------+\n",
      "|  count|             13482|\n",
      "|   mean| 73.83859961430055|\n",
      "| stddev|118.50547311680688|\n",
      "|    min|            -994.0|\n",
      "|    max|            1283.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_max = weather[weather['element'] == 'TMAX']\n",
    "t_max[['value']].describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Filtering - TMAX and Station: USC00114078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+-----------+-------+------+-----------+-----+\n",
      "|    date|element|  hour|measurement|quality|source|    station|value|\n",
      "+--------+-------+------+-----------+-------+------+-----------+-----+\n",
      "|20070101|   TMAX|1800.0|           |       |      |USC00114078|128.0|\n",
      "|20070102|   TMAX|1800.0|           |       |      |USC00114078| 83.0|\n",
      "+--------+-------+------+-----------+-------+------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_station_tmax = weather[(weather['element'] == 'TMAX') & (weather['station'] == 'USC00114078')]\n",
    "one_station_tmax.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Read that 1 unit TMAX is 0.1 degrees celcius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as Func\n",
    "one_station_tmax = one_station_tmax.withColumn('reset_value', Func.lit(0.1))\n",
    "one_station_tmax = one_station_tmax.withColumn('tmax_celc', one_station_tmax.reset_value * one_station_tmax.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+-----------+-------+------+-----------+-----+-----------+---------+\n",
      "|    date|element|  hour|measurement|quality|source|    station|value|reset_value|tmax_celc|\n",
      "+--------+-------+------+-----------+-------+------+-----------+-----+-----------+---------+\n",
      "|20070101|   TMAX|1800.0|           |       |      |USC00114078|128.0|        0.1|     12.8|\n",
      "|20070102|   TMAX|1800.0|           |       |      |USC00114078| 83.0|        0.1|      8.3|\n",
      "+--------+-------+------+-----------+-------+------+-----------+-----+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_station_tmax.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Look at min, max, avg by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_station_tmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f5fcee7586f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_station_tmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'one_station_tmax' is not defined"
     ]
    }
   ],
   "source": [
    "one_station_tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+\n",
      "|    date|max(value)|min(value)|avg(value)|\n",
      "+--------+----------+----------+----------+\n",
      "|20070101|     128.0|     128.0|     128.0|\n",
      "|20070102|      83.0|      83.0|      83.0|\n",
      "+--------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_station_tmax.groupby('date').agg(Func.max('value'),Func.min('value'), Func.mean('value')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Google Cloud\n",
    "Find that Google Cloud offers the first two months free.  So you can do something real with MLlib.\n",
    "\n",
    "- bdutil\n",
    "    - https://cloud.google.com/hadoop/setting-up-a-hadoop-cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Spin up 4 worker nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Set up ports to allow Spark UI\n",
    "- Viewed through port 4040\n",
    "- Create tag for ports\n",
    "    - gcloud compute firewall-rules create default-allow-8080 --description=\"Incoming http 8080 allowed.\" â€”allow=\"tcp:4040\" --allow=\"tcp:8080\" --allow=\"tcp:8081\" --target-tags=\"http-8080-server\" \n",
    "- Need to open ports  \n",
    "    - gcloud compute instances add-tags hadoop-m --zone us-central1-a --tags http-8080-server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
